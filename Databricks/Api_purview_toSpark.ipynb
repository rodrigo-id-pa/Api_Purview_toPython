{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBS: \n",
    "# Precisa do azure configurado\n",
    "# Precisa do purview configurado no azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from pyapacheatlas.auth import ServicePrincipalAuthentication\n",
    "from pyapacheatlas.core import PurviewClient, AtlasEntity, AtlasProcess, TypeCategory\n",
    "from pyapacheatlas.core.typedef import *\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para autenticar a entidade de serviço para o URL de recurso fornecido e retorna o token oauth2 de acesso\n",
    "def azuread_auth(tenant_id: str, client_id: str, client_secret: str, resource_url: str):\n",
    "    \n",
    "    url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "    payload= f'grant_type=client_credentials&client_id={client_id}&client_secret={client_secret}&resource={resource_url}'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded'\n",
    "    }\n",
    "    \n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    access_token = json.loads(response.text)['access_token']\n",
    "    \n",
    "    return access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para autenticar no Atlas Endpoint e retorna um objeto Client\n",
    "def purview_auth(tenant_id: str, client_id: str, client_secret: str, data_catalog_name: str):\n",
    "  \n",
    "    oauth2 = ServicePrincipalAuthentication(\n",
    "        tenant_id = tenant_id,\n",
    "        client_id = client_id,\n",
    "        client_secret = client_secret\n",
    "    )\n",
    "    client = PurviewClient(\n",
    "        account_name = data_catalog_name,\n",
    "        authentication = oauth2\n",
    "    )\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir conexão com o datalake\n",
    "spark.conf.set(\n",
    "  \"\", # Endpoint do datalake\n",
    "  \"\" # Acess Key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criar json com as chaves de acesso do azure e purview\n",
    "\n",
    "# Recuperar caminho até o json de parâmetros da Autentificação do Purview\n",
    "path_params = dbutils.fs.ls('''path do json no azure''')\n",
    "\n",
    "# Recuperar dados do json de parâmetros da Autentificação do Purview\n",
    "parameters = spark.read.format(\"json\").load(path_params)\n",
    "\n",
    "client_id = parameters.select('client_id').collect()[0][0]\n",
    "client_secret = parameters.select('client_secret').collect()[0][0]\n",
    "data_catalog_name = parameters.select('data_catalog_name').collect()[0][0]\n",
    "resource_url = parameters.select('resource_url').collect()[0][0]\n",
    "tenant_id = parameters.select('tenant_id').collect()[0][0]\n",
    "\n",
    "#pegando data atual e formatando\n",
    "data_today = datetime.datetime.today() - timedelta(hours=3, minutes=0)\n",
    "data_now_str = data_today.strftime(\"%A %d %B %y %H:%M\")\n",
    "date_now = datetime.datetime.strptime(data_now_str, \"%A %d %B %y %H:%M\")\n",
    "\n",
    "# Recuperar objetos de autenticação\n",
    "azuread_access_token = azuread_auth(tenant_id, client_id, client_secret, resource_url)\n",
    "purview_client = purview_auth(tenant_id, client_id, client_secret, data_catalog_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requestId(endpoint: str, url: str, response: any, payload: any, headers: any):\n",
    "    endpoint = endpoint\n",
    "    url = url\n",
    "    payload = payload\n",
    "    headers = headers\n",
    "    response = response\n",
    "\n",
    "    if(response.status_code != 200):\n",
    "      print(\"Status:\", response.status_code, \"Erro no código\", response.text)\n",
    "    else:\n",
    "      print(\"Status:\", response.status_code, \"Ok\", response.text)\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buscando dados da coleção\n",
    "endpoint = \"https://\"+data_catalog_name+\".purview.azure.com/\"\n",
    "url = f\"{endpoint}/catalog/api/search/query?api-version=2021-05-01-preview\"\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {azuread_access_token}'\n",
    "   }\n",
    "payload = json.dumps({\n",
    "        \"orderby\":[\"name\"],\n",
    "        \"limit\": 1000,\n",
    "        \"keywords\": None,\n",
    "        \"filter\": {\n",
    "            \"and\": [\n",
    "              {\n",
    "                \"or\": [\n",
    "                  {\n",
    "                    \"collectionId\": \"id da coleção aqui\"\n",
    "                  }\n",
    "                ]\n",
    "              },\n",
    "              {\n",
    "                \"or\": [\n",
    "                  {\n",
    "                    \"classification\": \"classificação\",\n",
    "                    \"includeSubClassifications\": True\n",
    "                  },\n",
    "                  {\n",
    "                      \"classification\": \"classificação\",\n",
    "                    \"includeSubClassifications\": True\n",
    "                  }\n",
    "                ]\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        })\n",
    "response_dp = requests.post(url, headers=headers, data=payload)\n",
    "\n",
    "requestId(endpoint, url, response_dp, headers, payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataframe com todos os dados\n",
    "df_value = pd.DataFrame(json.loads(response_dp.text)['value'])\n",
    "\n",
    "## listando cada id de cada tabela\n",
    "df_id = df_value['id']\n",
    "\n",
    "## json com todas as tabelas e colunas\n",
    "list_response = []\n",
    "for id_ in df_id:\n",
    "    endpoint = \"https://\"+data_catalog_name+\".purview.azure.com/\"\n",
    "    url = f\"{endpoint}/catalog/api/atlas/v2/entity/bulk?excludeRelationshipTypes=dataset_process_inputs&excludeRelationshipTypes=process_parent&excludeRelationshipTypes=direct_lineage_dataset_dataset&guid={id_}&includeTermsInMinExtInfo=false&minExtInfo=false&ignoreRelationships=false\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {azuread_access_token}'\n",
    "    }\n",
    "    payload = azuread_access_token\n",
    "\n",
    "    response = requests.get(url, headers=headers, data=payload)\n",
    "    requestId(endpoint, url, response, headers, payload)\n",
    "    list_response.append(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataframe com todos os dados\n",
    "df_list = pd.DataFrame(list_response)\n",
    "\n",
    "## percorrendo e listando cada coluna no dataframe para buscar nomes e classificações\n",
    "\n",
    "df_list_res_referredEntities = pd.DataFrame()\n",
    "\n",
    "for x in list_response:\n",
    "    df_list = pd.DataFrame(json.loads(x)['referredEntities']).reindex(['attributes','classifications']).transpose().dropna()\n",
    "    df_list_res_referredEntities = pd.concat([df_list_res_referredEntities, df_list])\n",
    "\n",
    "df_list_res_referredEntities.reset_index(inplace=True)\n",
    "df_list_res_referredEntities = df_list_res_referredEntities.rename(columns = {'index':'IDs'})\n",
    "\n",
    "## listando dataframe com nome das colunas e criando nova coluna para filtrar as classificações\n",
    "df_columns = df_list_res_referredEntities\n",
    "df_columns['classificação'] = ''\n",
    "df_columns['data'] = date_now\n",
    "\n",
    "for i in range(df_columns.shape[0]):\n",
    "    df_columns.loc[i,'attributes'] = df_columns.loc[i,'attributes']['name']\n",
    "    df_columns.loc[:,'classificação'].iloc[i] = df_columns.classifications.iloc[i][0]['typeName']  \n",
    "\n",
    "## dataframe com colunas e suas classificações\n",
    "df_col = df_columns.loc[:, ['IDs','attributes', 'classificação', 'data']]\n",
    "\n",
    "## dataframe com tabelas e suas classificações\n",
    "df_tables = df_value.loc[:, ['id','name', 'classification']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe com nome das tabelas e filtrando as classificações para nova tabela\n",
    "df_tables['classificação2'] = ''\n",
    "df_tables['data2'] = date_now\n",
    "\n",
    "for counter, list_ in enumerate(df_tables['classification']):\n",
    "    if('nome da classificação aqui' in list_ and 'nome da classificação aqui' in list_):\n",
    "        df_tables.loc[counter,\n",
    "                      'nome da classificação aqui'] = 'nome da classificação aqui'\n",
    "    elif 'nome da classificação aqui' in list_:\n",
    "        df_tables.loc[counter,\n",
    "                      'nome da classificação aqui'] = 'nome da classificação aqui'\n",
    "    elif 'nome da classificação aqui' in list_:\n",
    "        df_tables[counter, 'nome da classificação aqui'] = 'nome da classificação aqui'\n",
    "\n",
    "## dataframe com tabelas e suas classificações\n",
    "df_tab = df_tables.loc[:, ['id', 'name', 'classificação2', 'data2']].replace('', 'Tabela não classificada')\n",
    "\n",
    "# 1 - criando dataframe\n",
    "# 2 - trazendo id, nome e classificação das tabelas\n",
    "# 3 - criando nova coluna com o id das colunas\n",
    "# 4 - igualando o id da tabela com a coluna para fazer o merge\n",
    "\n",
    "df_tab.loc[:,'Id_simplificado'] = ''\n",
    "for count, item in enumerate(df_tab.id):\n",
    "    df_tab.loc[:,'Id_simplificado'].iloc[count] = item[:33]\n",
    "\n",
    "df_col.loc[:,'Id_simplificado'] = ''    \n",
    "for count, item in enumerate(df_col.IDs):\n",
    "    df_col.loc[:,'Id_simplificado'].iloc[count] = item[:33]    \n",
    "    \n",
    "## dataframe final com todas as tabelas e suas colunas com id, nome e classificação\n",
    "tb_merge = df_tab.merge(df_col, left_on='Id_simplificado', right_on='Id_simplificado')\n",
    "\n",
    "## dataframe final com tabelas e colunas classificadas\n",
    "tb_data = tb_merge[['name', 'classificação2', 'attributes', 'classificação', 'data']].rename(columns={'name':'novo nome', 'classificação2':'novo nome', 'attributes':'novo nome', 'classificação':'novo nome', 'data': 'novo nome'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabela final com tabelas e colunas com apenas classificação\n",
    "df = tb_data.loc[((tb_data['novo nome'] == 'sua classificação aqui') | (\n",
    "    tb_data['novo nome'] == 'sua classificação aqui'))]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para exportar na pasta temporária databricks e depois mover para pasta no azure\n",
    "def export_file(dataframe, ext_target, target, new_name):\n",
    "  df_res = dataframe\n",
    "  df_res.to_csv('/dbfs/tmp/nome da pasta/'+new_name+'.' +\n",
    "                ext_target, sep=',', index=False)\n",
    "  dbutils.fs.mv(\"/tmp/nome da pasta/\"+new_name+'.'+ext_target,\n",
    "                target+'/'+new_name+'.'+ext_target)\n",
    "  print('Novo arquivo {} salvo em {}'.format(new_name, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar arquivo json ou txt em csv no container landing\n",
    "dataframe = df_list\n",
    "ext_target = \"csv\"\n",
    "path_raw = 'path do landing no azure'\n",
    "new_name = f\"nome do arquivo\"\n",
    "export_file(dataframe, ext_target, path_raw, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar arquivo csv no container raw\n",
    "dataframe = df\n",
    "ext_target = \"csv\"\n",
    "path_raw = 'path do raw no azure'\n",
    "new_name = f\"nome do arquivo\"\n",
    "export_file(dataframe, ext_target, path_raw, new_name)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
